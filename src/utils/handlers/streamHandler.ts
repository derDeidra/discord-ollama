import { ChatResponse } from "ollama"
import { ChatParams } from "../index.js"
import { AbortableAsyncIterator } from "ollama/src/utils.js"

/**
 * Method to query the Ollama client for async generation
 * @param params 
 * @returns AsyncIterator<ChatResponse> generated by the Ollama client
 */
export async function streamResponse(params: ChatParams): Promise<AbortableAsyncIterator<ChatResponse>> {
    return await params.ollama.chat({
        model: params.model,
        messages: params.msgHist,
        options: {
            mirostat: 1,
            mirostat_tau: 2.0,
            top_k: 70
        },
        stream: true
    }) as unknown as AbortableAsyncIterator<ChatResponse>
}

/**
 * Method to query the Ollama client for a block response 
 * @param params parameters to query the client
 * @returns ChatResponse generated by the Ollama client
 */
export async function blockResponse(params: ChatParams): Promise<ChatResponse> {
    // Inject user ID prior to sending since ollama does not support it natively
    params.msgHist = params.msgHist.map(msg => {
        if (msg.role !== 'user') {
            return msg
        }
        return {
            ...msg,
            content: `[user ${msg.userId}]: ${msg.content}`
        }
    })
    return await params.ollama.chat({
        model: params.model,
        messages: params.msgHist,
        options: {
            temperature: 0.8,
            top_p: 0.92,
            top_k: 60,
            // typical_p if supported by your client/runtime:
            typical_p: 0.98,
            repeat_penalty: 1.2,        // not 2.0
            repeat_last_n: 768,         // apply penalty over a longer recent window
            frequency_penalty: 0.35,
            presence_penalty: 0.15,
        },
        stream: false
    })
}